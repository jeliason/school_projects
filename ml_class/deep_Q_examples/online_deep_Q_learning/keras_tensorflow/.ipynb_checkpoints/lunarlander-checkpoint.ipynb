{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "\n",
    "class QLearner():\n",
    "    # load in simulator, initialize global variables\n",
    "    def __init__(self,simulator,savename,**kwargs):\n",
    "        # make simulator global\n",
    "        self.simulator = simulator\n",
    "        \n",
    "        # Q learn params\n",
    "        self.explore_val = 1\n",
    "        self.explore_decay = 0.99\n",
    "        self.num_episodes = 500        \n",
    "        self.gamma = 1\n",
    "        \n",
    "        if \"gamma\" in kwargs:   \n",
    "            self.gamma = args['gamma']\n",
    "        if 'explore_val' in kwargs:\n",
    "            self.explore_val = kwargs['explore_val']\n",
    "        if 'explore_decay' in kwargs:\n",
    "            self.explore_decay = kwargs['explore_decay']\n",
    "        if 'num_episodes' in kwargs:\n",
    "            self.num_episodes = kwargs['num_episodes']\n",
    "            \n",
    "        # other training variables\n",
    "        self.num_actions = self.simulator.action_space.n\n",
    "        state = self.simulator.reset()    \n",
    "        self.state_dim = np.size(state)\n",
    "        self.training_reward = []\n",
    "        \n",
    "        # setup memory params\n",
    "        self.memory_length = 2000    # length of memory replay\n",
    "        self.replay_length = 200     # length of replay sample\n",
    "        self.memory_start = 1000\n",
    "        self.memory = []\n",
    "        if 'memory_length' in kwargs:\n",
    "            self.memory_length = kwargs['memory_length']\n",
    "        if 'replay_length' in kwargs:\n",
    "            self.replay_length = kwargs['replay_length']\n",
    "        if 'memory_start' in kwargs:\n",
    "            self.memory_start = kwargs['memory_start']\n",
    "        \n",
    "        ### initialize logs ###\n",
    "        # create text file for training log\n",
    "        self.logname = 'training_logs/' + savename + '.txt'\n",
    "        self.reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "        self.weight_name = 'saved_model_weights/' + savename + '.pkl' \n",
    "        self.model_name = 'models/' + savename + '.json'\n",
    "\n",
    "        self.init_log(self.logname)\n",
    "        self.init_log(self.reward_logname)\n",
    "        self.init_log(self.weight_name)\n",
    "        self.init_log(self.model_name)\n",
    "     \n",
    "    ##### logging functions #####\n",
    "    def init_log(self,logname):\n",
    "        # delete log if old version exists\n",
    "        if os.path.exists(logname): \n",
    "            os.remove(logname)\n",
    "            \n",
    "    def update_log(self,logname,update):\n",
    "        if type(update) == str:\n",
    "            logfile = open(logname, \"a\")\n",
    "            logfile.write(update)\n",
    "            logfile.close() \n",
    "        else:\n",
    "            weights = []\n",
    "            if os.path.exists(logname):\n",
    "                with open(logname,'rb') as rfp: \n",
    "                    weights = pickle.load(rfp)\n",
    "            weights.append(update)\n",
    "\n",
    "            with open(logname,'wb') as wfp:\n",
    "                pickle.dump(weights, wfp)\n",
    "    \n",
    "    ##### functions for creating / updating Q #####\n",
    "    def initialize_Q(self,**kwargs):\n",
    "        # default parameters for network\n",
    "        layer_sizes = [10,10]      # two hidden layers, 10 units each, by default\n",
    "        activation = 'relu'\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        if 'activation' in kwargs:\n",
    "            activation = kwargs['activation']\n",
    "\n",
    "        # default parameters for optimizer - reset by hand\n",
    "        loss = 'mse'\n",
    "        lr = 10**(-2)\n",
    "        if 'alpha' in kwargs:\n",
    "            lr = kwargs['alpha']\n",
    "        optimizer = RMSprop(lr = lr)        \n",
    "\n",
    "        # input / output sizes of network\n",
    "        input_dim = self.state_dim\n",
    "        output_dim = self.num_actions\n",
    "\n",
    "        # build model based on parameters\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # add input layer\n",
    "        self.model.add(Dense(layer_sizes[0], input_dim=input_dim, activation=activation))\n",
    "\n",
    "        # add hidden layers\n",
    "        for U in layer_sizes[1:]:\n",
    "            self.model.add(Dense(U,activation=activation))\n",
    "\n",
    "        # add output layer\n",
    "        self.model.add(Dense(output_dim, activation='linear'))\n",
    "\n",
    "        # chose optimizer and its associated parameters\n",
    "        self.model.compile(loss=loss, optimizer=optimizer)\n",
    "        \n",
    "        # initialize Q\n",
    "        self.Q = self.model.predict\n",
    "        \n",
    "        # since its easy to save models with keras / tensorflow, save to file\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.model_name, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "    # update Q function\n",
    "    def update_Q(self,state,next_state,action,reward,done):\n",
    "        # add newest sample to queue\n",
    "        self.update_memory(state,next_state,action,reward,done)\n",
    "        \n",
    "        # only update Q if sufficient memory has been collected\n",
    "        if len(self.memory) < self.memory_start:\n",
    "            return\n",
    "        \n",
    "        # update memory sample\n",
    "        self.sample_memory()\n",
    "        \n",
    "        # generate q_values based on most recent Q\n",
    "        q_vals = []\n",
    "        states = []\n",
    "        for i in range(len(self.replay_samples)):    \n",
    "            # get sample\n",
    "            sample = self.replay_samples[i]\n",
    "            \n",
    "            # strip sample for parts\n",
    "            state = sample[0]\n",
    "            next_state = sample[1]\n",
    "            action = sample[2]\n",
    "            reward = sample[3]\n",
    "            done = sample[4]\n",
    "                            \n",
    "            ### for cartpole only - check if done, and alter reward to improve learning ###\n",
    "            #done,reward = self.check_done(done,reward)\n",
    "\n",
    "            # compute and store q value\n",
    "            q = reward \n",
    "            if done == False:\n",
    "                qs = self.Q(next_state)\n",
    "                q += self.gamma*np.max(qs)\n",
    "            \n",
    "            # clamp all other models to their current values for this input/output pair\n",
    "            q_update = self.Q(state).flatten()\n",
    "            q_update[action] = q\n",
    "            q_vals.append(q_update)\n",
    "            states.append(state.T)\n",
    "            \n",
    "        # convert lists to numpy arrays for regressor\n",
    "        s_in = np.array(states).T\n",
    "        q_vals = np.array(q_vals).T\n",
    "        s_in = s_in[0,:,:]\n",
    "                            \n",
    "        # take descent step\n",
    "        num_pts = s_in.shape[1]\n",
    "        self.model.fit(x=s_in.T,y=q_vals.T,batch_size=num_pts, epochs=1,verbose = 0)\n",
    "        \n",
    "        # update Q based on regressor updates\n",
    "        self.Q = self.model.predict\n",
    "        \n",
    "    ##### functions for adjusting replay memory #####\n",
    "    # update memory - add sample to list, remove oldest samples \n",
    "    def update_memory(self,state,next_state,action,reward,done):\n",
    "        # add most recent trial data to memory\n",
    "        self.memory.append([state,next_state,action,reward,done])\n",
    "\n",
    "        # clip memory if it gets too long    \n",
    "        num_elements = len(self.memory)\n",
    "        if num_elements >= self.memory_length:    \n",
    "            num_delete = num_elements - self.memory_length\n",
    "            self.memory[:num_delete] = []\n",
    "    \n",
    "    # sample from memory and create input / output pairs for regression\n",
    "    def sample_memory(self):\n",
    "        # indices to sample\n",
    "        memory_num = len(self.memory)\n",
    "        sample_nums = np.random.permutation(memory_num)[:self.replay_length]\n",
    "        \n",
    "        # create samples\n",
    "        self.replay_samples = [self.memory[v] for v in sample_nums]\n",
    "    \n",
    "    ##### Q Learning functionality #####\n",
    "    # special function to check done\n",
    "    def check_done(self,done,reward):\n",
    "        if done == True:\n",
    "            reward = -100\n",
    "        return done,reward\n",
    "    \n",
    "    # state normalizer\n",
    "    def state_normalizer(self,states):\n",
    "        states = np.array(states)[np.newaxis,:]\n",
    "        return states\n",
    "    \n",
    "    # choose next action\n",
    "    def choose_action(self,state):\n",
    "        # pick action at random\n",
    "        p = np.random.rand(1)   \n",
    "        action = np.random.randint(self.num_actions)\n",
    "            \n",
    "        # pick action based on exploiting\n",
    "        if len(self.memory) >= self.memory_start:\n",
    "            qs = self.Q(state) \n",
    "            if p > self.explore_val:\n",
    "                action = np.argmax(qs)\n",
    "        return action\n",
    "    \n",
    "    # main training function\n",
    "    def train(self,**kwargs):        \n",
    "        ### start main Q-learning loop ###\n",
    "        for n in range(self.num_episodes): \n",
    "            # pick this episode's starting position - randomly initialize from f_system\n",
    "            state = self.simulator.reset()    \n",
    "            state = self.state_normalizer(state)\n",
    "            total_episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # get out exploit parameter for this episode\n",
    "            if self.explore_val > 0.01:\n",
    "                self.explore_val *= self.explore_decay\n",
    "                    \n",
    "            # run episode\n",
    "            step = 0\n",
    "            while done == False:    \n",
    "                # choose next action\n",
    "                action = self.choose_action(state)\n",
    "    \n",
    "                # transition to next state, get associated reward\n",
    "                next_state,reward,done,info = self.simulator.step(action)  \n",
    "                next_state = self.state_normalizer(next_state)\n",
    "                \n",
    "                # update Q function\n",
    "                self.update_Q(state,next_state,action,reward,done)  \n",
    "\n",
    "                # update total reward from this episode\n",
    "                total_episode_reward+=reward\n",
    "                state = copy.deepcopy(next_state)\n",
    "                  \n",
    "            # start storing once memory is full \n",
    "            if len(self.memory) >= self.memory_start:\n",
    "                # print out update if verbose set to True\n",
    "                update = 'training episode ' + str(n+1) +  ' of ' + str(self.num_episodes) + ' complete, ' +  ' explore val = ' + str(np.round(self.explore_val,3)) + ', episode reward = ' + str(np.round(total_episode_reward,2)) \n",
    "\n",
    "                self.update_log(self.logname,update + '\\n')\n",
    "                print (update)\n",
    "\n",
    "                update = str(total_episode_reward) + '\\n'\n",
    "                self.update_log(self.reward_logname,update)\n",
    "\n",
    "                ### store this episode's computation time and training reward history\n",
    "                self.training_reward.append(total_episode_reward)\n",
    "\n",
    "                # save latest weights from this episode \n",
    "                update = self.model.get_weights()\n",
    "                self.update_log(self.weight_name,update)\n",
    "            \n",
    "        ### save weights ###\n",
    "        update = 'q-learning algorithm complete'\n",
    "        self.update_log(self.logname,update + '\\n')\n",
    "        print (update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "\n",
    "# savename\n",
    "savename = 'lunarlander_experiment_1'\n",
    "\n",
    "# initialize simulator\n",
    "cartpole = gym.make('LunarLander-v2') \n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 10000\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "\n",
    "# initialize memory\n",
    "replay_length = 100\n",
    "memory_length = 1000\n",
    "\n",
    "# load into instance of learner\n",
    "demo = QLearner(cartpole,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,replay_length=replay_length)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [50,50]\n",
    "alpha = 10**(-2)\n",
    "activation = 'tanh'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 12 of 10000 complete,  explore val = 0.01, episode reward = -433.79\n",
      "training episode 13 of 10000 complete,  explore val = 0.01, episode reward = -344.68\n",
      "training episode 14 of 10000 complete,  explore val = 0.01, episode reward = -193.77\n",
      "training episode 15 of 10000 complete,  explore val = 0.01, episode reward = -405.29\n",
      "training episode 16 of 10000 complete,  explore val = 0.01, episode reward = -96.43\n",
      "training episode 17 of 10000 complete,  explore val = 0.01, episode reward = -166.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3199fc223f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ae394e8db6b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;31m# update Q function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;31m# update total reward from this episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ae394e8db6b0>\u001b[0m in \u001b[0;36mupdate_Q\u001b[0;34m(self, state, next_state, action, reward, done)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# clamp all other models to their current values for this input/output pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mq_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mq_update\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mq_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1573\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1201\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_reward_history(logname,**kwargs):\n",
    "    # load in total episode reward history\n",
    "    data = np.loadtxt(logname)\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(figsize = (12,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    # plot total reward history\n",
    "    start = 0\n",
    "    if 'start' in kwargs:\n",
    "        start = kwargs['start']\n",
    "    ax.plot(data[start:])\n",
    "    ax.set_xlabel('episode')\n",
    "    ax.set_ylabel('total reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plot_reward_history(reward_logname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# produce animation of validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in weights\n",
    "infile = demo.weight_name\n",
    "with open(infile, 'rb') as in_strm:\n",
    "    datastruct = pickle.load(in_strm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
