{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "\n",
    "class QLearner():\n",
    "    # load in simulator, initialize global variables\n",
    "    def __init__(self,simulator,savename,**kwargs):\n",
    "        # make simulator global\n",
    "        self.simulator = simulator\n",
    "        \n",
    "        # Q learn params\n",
    "        self.explore_val = 1\n",
    "        self.explore_decay = 0.99\n",
    "        self.num_episodes = 500        \n",
    "        self.gamma = 1\n",
    "        \n",
    "        if \"gamma\" in kwargs:   \n",
    "            self.gamma = args['gamma']\n",
    "        if 'explore_val' in kwargs:\n",
    "            self.explore_val = kwargs['explore_val']\n",
    "        if 'explore_decay' in kwargs:\n",
    "            self.explore_decay = kwargs['explore_decay']\n",
    "        if 'num_episodes' in kwargs:\n",
    "            self.num_episodes = kwargs['num_episodes']\n",
    "            \n",
    "        # other training variables\n",
    "        self.num_actions = self.simulator.action_space.n\n",
    "        state = self.simulator.reset()    \n",
    "        self.state_dim = np.size(state)\n",
    "        self.training_reward = []\n",
    "        \n",
    "        # setup memory params\n",
    "        self.memory_length = 2000    # length of memory replay\n",
    "        self.replay_length = 200     # length of replay sample\n",
    "        self.memory_start = 1000\n",
    "        self.memory = []\n",
    "        if 'memory_length' in kwargs:\n",
    "            self.memory_length = kwargs['memory_length']\n",
    "        if 'replay_length' in kwargs:\n",
    "            self.replay_length = kwargs['replay_length']\n",
    "        if 'memory_start' in kwargs:\n",
    "            self.memory_start = kwargs['memory_start']\n",
    "        \n",
    "        ### initialize logs ###\n",
    "        # create text file for training log\n",
    "        self.logname = 'training_logs/' + savename + '.txt'\n",
    "        self.reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "        self.weight_name = 'saved_model_weights/' + savename + '.pkl' \n",
    "        self.model_name = 'models/' + savename + '.json'\n",
    "\n",
    "        self.init_log(self.logname)\n",
    "        self.init_log(self.reward_logname)\n",
    "        self.init_log(self.weight_name)\n",
    "        self.init_log(self.model_name)\n",
    "     \n",
    "    ##### logging functions #####\n",
    "    def init_log(self,logname):\n",
    "        # delete log if old version exists\n",
    "        if os.path.exists(logname): \n",
    "            os.remove(logname)\n",
    "            \n",
    "    def update_log(self,logname,update):\n",
    "        if type(update) == str:\n",
    "            logfile = open(logname, \"a\")\n",
    "            logfile.write(update)\n",
    "            logfile.close() \n",
    "        else:\n",
    "            weights = []\n",
    "            if os.path.exists(logname):\n",
    "                with open(logname,'rb') as rfp: \n",
    "                    weights = pickle.load(rfp)\n",
    "            weights.append(update)\n",
    "\n",
    "            with open(logname,'wb') as wfp:\n",
    "                pickle.dump(weights, wfp)\n",
    "    \n",
    "    ##### functions for creating / updating Q #####\n",
    "    def initialize_Q(self,**kwargs):\n",
    "        # default parameters for network\n",
    "        layer_sizes = [10,10]      # two hidden layers, 10 units each, by default\n",
    "        activation = 'relu'\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        if 'activation' in kwargs:\n",
    "            activation = kwargs['activation']\n",
    "\n",
    "        # default parameters for optimizer - reset by hand\n",
    "        loss = 'mse'\n",
    "        lr = 10**(-2)\n",
    "        if 'alpha' in kwargs:\n",
    "            lr = kwargs['alpha']\n",
    "        optimizer = RMSprop(lr = lr)        \n",
    "\n",
    "        # input / output sizes of network\n",
    "        input_dim = self.state_dim\n",
    "        output_dim = self.num_actions\n",
    "\n",
    "        # build model based on parameters\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # add input layer\n",
    "        self.model.add(Dense(layer_sizes[0], input_dim=input_dim, activation=activation))\n",
    "\n",
    "        # add hidden layers\n",
    "        for U in layer_sizes[1:]:\n",
    "            self.model.add(Dense(U,activation=activation))\n",
    "\n",
    "        # add output layer\n",
    "        self.model.add(Dense(output_dim, activation='linear'))\n",
    "\n",
    "        # chose optimizer and its associated parameters\n",
    "        self.model.compile(loss=loss, optimizer=optimizer)\n",
    "        \n",
    "        # initialize Q\n",
    "        self.Q = self.model.predict\n",
    "        \n",
    "        # since its easy to save models with keras / tensorflow, save to file\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.model_name, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "    # update Q function\n",
    "    def update_Q(self,state,next_state,action,reward,done):\n",
    "        # add newest sample to queue\n",
    "        self.update_memory(state,next_state,action,reward,done)\n",
    "        \n",
    "        # only update Q if sufficient memory has been collected\n",
    "        if len(self.memory) < self.memory_start:\n",
    "            return\n",
    "        \n",
    "        # update memory sample\n",
    "        self.sample_memory()\n",
    "        \n",
    "        # generate q_values based on most recent Q\n",
    "        q_vals = []\n",
    "        states = []\n",
    "        for i in range(len(self.replay_samples)):    \n",
    "            # get sample\n",
    "            sample = self.replay_samples[i]\n",
    "            \n",
    "            # strip sample for parts\n",
    "            state = sample[0]\n",
    "            next_state = sample[1]\n",
    "            action = sample[2]\n",
    "            reward = sample[3]\n",
    "            done = sample[4]\n",
    "                            \n",
    "            ### for cartpole only - check if done, and alter reward to improve learning ###\n",
    "            done,reward = self.check_done(done,reward)\n",
    "\n",
    "            # compute and store q value\n",
    "            q = reward \n",
    "            if done == False:\n",
    "                qs = self.Q(next_state)\n",
    "                q += self.gamma*np.max(qs)\n",
    "            \n",
    "            # clamp all other models to their current values for this input/output pair\n",
    "            q_update = self.Q(state).flatten()\n",
    "            q_update[action] = q\n",
    "            q_vals.append(q_update)\n",
    "            states.append(state.T)\n",
    "            \n",
    "        # convert lists to numpy arrays for regressor\n",
    "        s_in = np.array(states).T\n",
    "        q_vals = np.array(q_vals).T\n",
    "        s_in = s_in[0,:,:]\n",
    "                            \n",
    "        # take descent step\n",
    "        num_pts = s_in.shape[1]\n",
    "        self.model.fit(x=s_in.T,y=q_vals.T,batch_size=num_pts, epochs=1,verbose = 0)\n",
    "        \n",
    "        # update Q based on regressor updates\n",
    "        self.Q = self.model.predict\n",
    "        \n",
    "    ##### functions for adjusting replay memory #####\n",
    "    # update memory - add sample to list, remove oldest samples \n",
    "    def update_memory(self,state,next_state,action,reward,done):\n",
    "        # add most recent trial data to memory\n",
    "        self.memory.append([state,next_state,action,reward,done])\n",
    "\n",
    "        # clip memory if it gets too long    \n",
    "        num_elements = len(self.memory)\n",
    "        if num_elements >= self.memory_length:    \n",
    "            num_delete = num_elements - self.memory_length\n",
    "            self.memory[:num_delete] = []\n",
    "    \n",
    "    # sample from memory and create input / output pairs for regression\n",
    "    def sample_memory(self):\n",
    "        # indices to sample\n",
    "        memory_num = len(self.memory)\n",
    "        sample_nums = np.random.permutation(memory_num)[:self.replay_length]\n",
    "        \n",
    "        # create samples\n",
    "        self.replay_samples = [self.memory[v] for v in sample_nums]\n",
    "    \n",
    "    ##### Q Learning functionality #####\n",
    "    # special function to check done\n",
    "    def check_done(self,done,reward):\n",
    "        if done == True:\n",
    "            reward = -100\n",
    "        return done,reward\n",
    "    \n",
    "    # state normalizer\n",
    "    def state_normalizer(self,states):\n",
    "        states = np.array(states)[np.newaxis,:]\n",
    "        return states\n",
    "    \n",
    "    # choose next action\n",
    "    def choose_action(self,state):\n",
    "        # pick action at random\n",
    "        p = np.random.rand(1)   \n",
    "        action = np.random.randint(self.num_actions)\n",
    "            \n",
    "        # pick action based on exploiting\n",
    "        if len(self.memory) >= self.memory_start:\n",
    "            qs = self.Q(state) \n",
    "            if p > self.explore_val:\n",
    "                action = np.argmax(qs)\n",
    "        return action\n",
    "    \n",
    "    # main training function\n",
    "    def train(self,**kwargs):        \n",
    "        ### start main Q-learning loop ###\n",
    "        for n in range(self.num_episodes): \n",
    "            # pick this episode's starting position - randomly initialize from f_system\n",
    "            state = self.simulator.reset()    \n",
    "            state = self.state_normalizer(state)\n",
    "            total_episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # get out exploit parameter for this episode\n",
    "            if self.explore_val > 0.01:\n",
    "                self.explore_val *= self.explore_decay\n",
    "                    \n",
    "            # run episode\n",
    "            step = 0\n",
    "            while done == False and step < 500:    \n",
    "                # choose next action\n",
    "                action = self.choose_action(state)\n",
    "    \n",
    "                # transition to next state, get associated reward\n",
    "                next_state,reward,done,info = self.simulator.step(action)  \n",
    "                next_state = self.state_normalizer(next_state)\n",
    "                \n",
    "                # update Q function\n",
    "                self.update_Q(state,next_state,action,reward,done)  \n",
    "\n",
    "                # update total reward from this episode\n",
    "                total_episode_reward+=reward\n",
    "                state = copy.deepcopy(next_state)\n",
    "                step+=1\n",
    "                  \n",
    "            # start storing once memory is full \n",
    "            if len(self.memory) >= self.memory_start:\n",
    "                # print out update if verbose set to True\n",
    "                update = 'training episode ' + str(n+1) +  ' of ' + str(self.num_episodes) + ' complete, ' +  ' explore val = ' + str(np.round(self.explore_val,3)) + ', episode reward = ' + str(np.round(total_episode_reward,2)) \n",
    "\n",
    "                self.update_log(self.logname,update + '\\n')\n",
    "                print (update)\n",
    "\n",
    "                update = str(total_episode_reward) + '\\n'\n",
    "                self.update_log(self.reward_logname,update)\n",
    "\n",
    "                ### store this episode's computation time and training reward history\n",
    "                self.training_reward.append(total_episode_reward)\n",
    "\n",
    "                # save latest weights from this episode \n",
    "                update = self.model.get_weights()\n",
    "                self.update_log(self.weight_name,update)\n",
    "            \n",
    "        ### save weights ###\n",
    "        update = 'q-learning algorithm complete'\n",
    "        self.update_log(self.logname,update + '\\n')\n",
    "        print (update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "\n",
    "# savename\n",
    "savename = 'cartpole_experiment_1'\n",
    "\n",
    "# initialize simulator\n",
    "simulator = gym.make('CartPole-v1') \n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 500\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "\n",
    "# initialize memory\n",
    "replay_length = 100\n",
    "memory_length = 1000\n",
    "\n",
    "# load into instance of learner\n",
    "demo = QLearner(simulator,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,replay_length=replay_length)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [50,50]\n",
    "alpha = 10**(-2)\n",
    "activation = 'relu'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 48 of 500 complete,  explore val = 0.01, episode reward = 27.0\n",
      "training episode 49 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 50 of 500 complete,  explore val = 0.01, episode reward = 78.0\n",
      "training episode 51 of 500 complete,  explore val = 0.01, episode reward = 256.0\n",
      "training episode 52 of 500 complete,  explore val = 0.01, episode reward = 142.0\n",
      "training episode 53 of 500 complete,  explore val = 0.01, episode reward = 111.0\n",
      "training episode 54 of 500 complete,  explore val = 0.01, episode reward = 245.0\n",
      "training episode 55 of 500 complete,  explore val = 0.01, episode reward = 122.0\n",
      "training episode 56 of 500 complete,  explore val = 0.01, episode reward = 118.0\n",
      "training episode 57 of 500 complete,  explore val = 0.01, episode reward = 117.0\n",
      "training episode 58 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 59 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 60 of 500 complete,  explore val = 0.01, episode reward = 178.0\n",
      "training episode 61 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 62 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 63 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 64 of 500 complete,  explore val = 0.01, episode reward = 88.0\n",
      "training episode 65 of 500 complete,  explore val = 0.01, episode reward = 181.0\n",
      "training episode 66 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 67 of 500 complete,  explore val = 0.01, episode reward = 162.0\n",
      "training episode 68 of 500 complete,  explore val = 0.01, episode reward = 38.0\n",
      "training episode 69 of 500 complete,  explore val = 0.01, episode reward = 34.0\n",
      "training episode 70 of 500 complete,  explore val = 0.01, episode reward = 263.0\n",
      "training episode 71 of 500 complete,  explore val = 0.01, episode reward = 20.0\n",
      "training episode 72 of 500 complete,  explore val = 0.01, episode reward = 24.0\n",
      "training episode 73 of 500 complete,  explore val = 0.01, episode reward = 18.0\n",
      "training episode 74 of 500 complete,  explore val = 0.01, episode reward = 24.0\n",
      "training episode 75 of 500 complete,  explore val = 0.01, episode reward = 21.0\n",
      "training episode 76 of 500 complete,  explore val = 0.01, episode reward = 26.0\n",
      "training episode 77 of 500 complete,  explore val = 0.01, episode reward = 96.0\n",
      "training episode 78 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 79 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 80 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 81 of 500 complete,  explore val = 0.01, episode reward = 136.0\n",
      "training episode 82 of 500 complete,  explore val = 0.01, episode reward = 236.0\n",
      "training episode 83 of 500 complete,  explore val = 0.01, episode reward = 472.0\n",
      "training episode 84 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 85 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 86 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 87 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 88 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 89 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 90 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 91 of 500 complete,  explore val = 0.01, episode reward = 85.0\n",
      "training episode 92 of 500 complete,  explore val = 0.01, episode reward = 16.0\n",
      "training episode 93 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 94 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 95 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 96 of 500 complete,  explore val = 0.01, episode reward = 15.0\n",
      "training episode 97 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 98 of 500 complete,  explore val = 0.01, episode reward = 18.0\n",
      "training episode 99 of 500 complete,  explore val = 0.01, episode reward = 20.0\n",
      "training episode 100 of 500 complete,  explore val = 0.01, episode reward = 99.0\n",
      "training episode 101 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 102 of 500 complete,  explore val = 0.01, episode reward = 452.0\n",
      "training episode 103 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 104 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 105 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 106 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 107 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 108 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 109 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 110 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 111 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 112 of 500 complete,  explore val = 0.01, episode reward = 8.0\n",
      "training episode 113 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 114 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 115 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 116 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 117 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 118 of 500 complete,  explore val = 0.01, episode reward = 61.0\n",
      "training episode 119 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 120 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 121 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 122 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 123 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 124 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 125 of 500 complete,  explore val = 0.01, episode reward = 8.0\n",
      "training episode 126 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 127 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 128 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 129 of 500 complete,  explore val = 0.01, episode reward = 18.0\n",
      "training episode 130 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 131 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 132 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 133 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 134 of 500 complete,  explore val = 0.01, episode reward = 367.0\n",
      "training episode 135 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 136 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 137 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 138 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 139 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 140 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 141 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 142 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 143 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 144 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 145 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 146 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 147 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 148 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 149 of 500 complete,  explore val = 0.01, episode reward = 101.0\n",
      "training episode 150 of 500 complete,  explore val = 0.01, episode reward = 32.0\n",
      "training episode 151 of 500 complete,  explore val = 0.01, episode reward = 22.0\n",
      "training episode 152 of 500 complete,  explore val = 0.01, episode reward = 22.0\n",
      "training episode 153 of 500 complete,  explore val = 0.01, episode reward = 19.0\n",
      "training episode 154 of 500 complete,  explore val = 0.01, episode reward = 22.0\n",
      "training episode 155 of 500 complete,  explore val = 0.01, episode reward = 19.0\n",
      "training episode 156 of 500 complete,  explore val = 0.01, episode reward = 18.0\n",
      "training episode 157 of 500 complete,  explore val = 0.01, episode reward = 19.0\n",
      "training episode 158 of 500 complete,  explore val = 0.01, episode reward = 37.0\n",
      "training episode 159 of 500 complete,  explore val = 0.01, episode reward = 101.0\n",
      "training episode 160 of 500 complete,  explore val = 0.01, episode reward = 229.0\n",
      "training episode 161 of 500 complete,  explore val = 0.01, episode reward = 282.0\n",
      "training episode 162 of 500 complete,  explore val = 0.01, episode reward = 155.0\n",
      "training episode 163 of 500 complete,  explore val = 0.01, episode reward = 153.0\n",
      "training episode 164 of 500 complete,  explore val = 0.01, episode reward = 133.0\n",
      "training episode 165 of 500 complete,  explore val = 0.01, episode reward = 61.0\n",
      "training episode 166 of 500 complete,  explore val = 0.01, episode reward = 47.0\n",
      "training episode 167 of 500 complete,  explore val = 0.01, episode reward = 39.0\n",
      "training episode 168 of 500 complete,  explore val = 0.01, episode reward = 33.0\n",
      "training episode 169 of 500 complete,  explore val = 0.01, episode reward = 34.0\n",
      "training episode 170 of 500 complete,  explore val = 0.01, episode reward = 42.0\n",
      "training episode 171 of 500 complete,  explore val = 0.01, episode reward = 47.0\n",
      "training episode 172 of 500 complete,  explore val = 0.01, episode reward = 59.0\n",
      "training episode 173 of 500 complete,  explore val = 0.01, episode reward = 85.0\n",
      "training episode 174 of 500 complete,  explore val = 0.01, episode reward = 118.0\n",
      "training episode 175 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 176 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 177 of 500 complete,  explore val = 0.01, episode reward = 362.0\n",
      "training episode 178 of 500 complete,  explore val = 0.01, episode reward = 234.0\n",
      "training episode 179 of 500 complete,  explore val = 0.01, episode reward = 168.0\n",
      "training episode 180 of 500 complete,  explore val = 0.01, episode reward = 116.0\n",
      "training episode 181 of 500 complete,  explore val = 0.01, episode reward = 81.0\n",
      "training episode 182 of 500 complete,  explore val = 0.01, episode reward = 61.0\n",
      "training episode 183 of 500 complete,  explore val = 0.01, episode reward = 79.0\n",
      "training episode 184 of 500 complete,  explore val = 0.01, episode reward = 102.0\n",
      "training episode 185 of 500 complete,  explore val = 0.01, episode reward = 106.0\n",
      "training episode 186 of 500 complete,  explore val = 0.01, episode reward = 86.0\n",
      "training episode 187 of 500 complete,  explore val = 0.01, episode reward = 122.0\n",
      "training episode 188 of 500 complete,  explore val = 0.01, episode reward = 150.0\n",
      "training episode 189 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 190 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 191 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 192 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 193 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 194 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 195 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 196 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 197 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 198 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 199 of 500 complete,  explore val = 0.01, episode reward = 91.0\n",
      "training episode 200 of 500 complete,  explore val = 0.01, episode reward = 39.0\n",
      "training episode 201 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 202 of 500 complete,  explore val = 0.01, episode reward = 464.0\n",
      "training episode 203 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-3199fc223f58>\", line 1, in <module>\n",
      "    demo.train()\n",
      "  File \"<ipython-input-1-c15d0ec87382>\", line 250, in train\n",
      "    self.update_Q(state,next_state,action,reward,done)\n",
      "  File \"<ipython-input-1-c15d0ec87382>\", line 162, in update_Q\n",
      "    q_update = self.Q(state).flatten()\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/keras/models.py\", line 899, in predict\n",
      "    return self.model.predict(x, batch_size=batch_size, verbose=verbose)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\", line 1573, in predict\n",
      "    batch_size=batch_size, verbose=verbose)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\", line 1203, in _predict_loop\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2103, in __call__\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 982, in _run\n",
      "    feed_dict_string, options, run_metadata)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\n",
      "    target_list, options, run_metadata)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1039, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1809, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.5/inspect.py\", line 715, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/opt/conda/lib/python3.5/inspect.py\", line 684, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/opt/conda/lib/python3.5/inspect.py\", line 669, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/opt/conda/lib/python3.5/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m"
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_reward_history(logname,**kwargs):\n",
    "    # load in total episode reward history\n",
    "    data = np.loadtxt(logname)\n",
    "    ave = [data[v] for v in range(100)]\n",
    "    \n",
    "    for i in range(0,np.size(data)-100):\n",
    "        m = np.mean(data[i:i+100])\n",
    "        ave.append(m)\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(figsize = (12,8))\n",
    "    ax1 = fig.add_subplot(2,1,1)\n",
    "    ax2 = fig.add_subplot(2,1,2)\n",
    "\n",
    "    # plot total reward history\n",
    "    start = 0\n",
    "    if 'start' in kwargs:\n",
    "        start = kwargs['start']\n",
    "    ax1.plot(data[start:])\n",
    "    ax1.set_xlabel('episode',labelpad = 8,fontsize = 13)\n",
    "    ax1.set_ylabel('total reward',fontsize = 13)\n",
    "    \n",
    "    ax2.plot(ave[start:],linewidth=3)\n",
    "    ax2.set_xlabel('episode',labelpad = 8,fontsize = 13)\n",
    "    ax2.set_ylabel('ave total reward',fontsize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plot_reward_history(reward_logname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
