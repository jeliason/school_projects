{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joele119/Desktop/rl_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "\n",
    "class QLearner():\n",
    "    # load in simulator, initialize global variables\n",
    "    def __init__(self,simulator,savename,**kwargs):\n",
    "        # make simulator global\n",
    "        self.simulator = simulator\n",
    "        \n",
    "        # Q learn params\n",
    "        self.explore_val = 1\n",
    "        self.explore_decay = 0.99\n",
    "        self.num_episodes = 500        \n",
    "        self.gamma = 1\n",
    "        \n",
    "        if \"gamma\" in kwargs:   \n",
    "            self.gamma = args['gamma']\n",
    "        if 'explore_val' in kwargs:\n",
    "            self.explore_val = kwargs['explore_val']\n",
    "        if 'explore_decay' in kwargs:\n",
    "            self.explore_decay = kwargs['explore_decay']\n",
    "        if 'num_episodes' in kwargs:\n",
    "            self.num_episodes = kwargs['num_episodes']\n",
    "            \n",
    "        # other training variables\n",
    "        self.num_actions = self.simulator.action_space.n\n",
    "        state = self.simulator.reset()    \n",
    "        self.state_dim = np.size(state)\n",
    "        self.training_reward = []\n",
    "        \n",
    "        # setup memory params\n",
    "        self.memory_length = 2000    # length of memory replay\n",
    "        self.replay_length = 200     # length of replay sample\n",
    "        self.memory_start = 1000\n",
    "        self.memory = []\n",
    "        if 'memory_length' in kwargs:\n",
    "            self.memory_length = kwargs['memory_length']\n",
    "        if 'replay_length' in kwargs:\n",
    "            self.replay_length = kwargs['replay_length']\n",
    "        if 'memory_start' in kwargs:\n",
    "            self.memory_start = kwargs['memory_start']\n",
    "        \n",
    "        ### initialize logs ###\n",
    "        # create text file for training log\n",
    "        self.logname = 'training_logs/' + savename + '.txt'\n",
    "        self.reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "        self.weight_name = 'saved_model_weights/' + savename + '.pkl' \n",
    "        self.model_name = 'models/' + savename + '.json'\n",
    "\n",
    "        self.init_log(self.logname)\n",
    "        self.init_log(self.reward_logname)\n",
    "        self.init_log(self.weight_name)\n",
    "        self.init_log(self.model_name)\n",
    "     \n",
    "    ##### logging functions #####\n",
    "    def init_log(self,logname):\n",
    "        # delete log if old version exists\n",
    "        if os.path.exists(logname): \n",
    "            os.remove(logname)\n",
    "            \n",
    "    def update_log(self,logname,update):\n",
    "        if type(update) == str:\n",
    "            logfile = open(logname, \"a\")\n",
    "            logfile.write(update)\n",
    "            logfile.close() \n",
    "        else:\n",
    "            weights = []\n",
    "            if os.path.exists(logname):\n",
    "                with open(logname,'rb') as rfp: \n",
    "                    weights = pickle.load(rfp)\n",
    "            weights.append(update)\n",
    "\n",
    "            with open(logname,'wb') as wfp:\n",
    "                pickle.dump(weights, wfp)\n",
    "    \n",
    "    ##### functions for creating / updating Q #####\n",
    "    def initialize_Q(self,**kwargs):\n",
    "        # default parameters for network\n",
    "        layer_sizes = [10,10]      # two hidden layers, 10 units each, by default\n",
    "        activation = 'relu'\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        if 'activation' in kwargs:\n",
    "            activation = kwargs['activation']\n",
    "\n",
    "        # default parameters for optimizer - reset by hand\n",
    "        loss = 'mse'\n",
    "        lr = 10**(-2)\n",
    "        if 'alpha' in kwargs:\n",
    "            lr = kwargs['alpha']\n",
    "        optimizer = RMSprop(lr = lr)        \n",
    "\n",
    "        # input / output sizes of network\n",
    "        input_dim = self.state_dim\n",
    "        output_dim = self.num_actions\n",
    "\n",
    "        # build model based on parameters\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # add input layer\n",
    "        self.model.add(Dense(layer_sizes[0], input_dim=input_dim, activation=activation))\n",
    "\n",
    "        # add hidden layers\n",
    "        for U in layer_sizes[1:]:\n",
    "            self.model.add(Dense(U,activation=activation))\n",
    "\n",
    "        # add output layer\n",
    "        self.model.add(Dense(output_dim, activation='linear'))\n",
    "\n",
    "        # chose optimizer and its associated parameters\n",
    "        self.model.compile(loss=loss, optimizer=optimizer)\n",
    "        \n",
    "        # initialize Q\n",
    "        self.Q = self.model.predict\n",
    "        \n",
    "        # since its easy to save models with keras / tensorflow, save to file\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.model_name, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "    # update Q function\n",
    "    def update_Q(self,state,next_state,action,reward,done):\n",
    "        # add newest sample to queue\n",
    "        self.update_memory(state,next_state,action,reward,done)\n",
    "        \n",
    "        # only update Q if sufficient memory has been collected\n",
    "        if len(self.memory) < self.memory_start:\n",
    "            return\n",
    "        \n",
    "        # update memory sample\n",
    "        self.sample_memory()\n",
    "        \n",
    "        # generate q_values based on most recent Q\n",
    "        q_vals = []\n",
    "        states = []\n",
    "        for i in range(len(self.replay_samples)):    \n",
    "            # get sample\n",
    "            sample = self.replay_samples[i]\n",
    "            \n",
    "            # strip sample for parts\n",
    "            state = sample[0]\n",
    "            next_state = sample[1]\n",
    "            action = sample[2]\n",
    "            reward = sample[3]\n",
    "            done = sample[4]\n",
    "                            \n",
    "            ### for cartpole only - check if done, and alter reward to improve learning ###\n",
    "            done,reward = self.check_done(done,reward)\n",
    "\n",
    "            # compute and store q value\n",
    "            q = reward \n",
    "            if done == False:\n",
    "                qs = self.Q(next_state)\n",
    "                q += self.gamma*np.max(qs)\n",
    "            \n",
    "            # clamp all other models to their current values for this input/output pair\n",
    "            q_update = self.Q(state).flatten()\n",
    "            q_update[action] = q\n",
    "            q_vals.append(q_update)\n",
    "            states.append(state.T)\n",
    "            \n",
    "        # convert lists to numpy arrays for regressor\n",
    "        s_in = np.array(states).T\n",
    "        q_vals = np.array(q_vals).T\n",
    "        s_in = s_in[0,:,:]\n",
    "                            \n",
    "        # take descent step\n",
    "        num_pts = s_in.shape[1]\n",
    "        self.model.fit(x=s_in.T,y=q_vals.T,batch_size=num_pts, epochs=1,verbose = 0)\n",
    "        \n",
    "        # update Q based on regressor updates\n",
    "        self.Q = self.model.predict\n",
    "        \n",
    "    ##### functions for adjusting replay memory #####\n",
    "    # update memory - add sample to list, remove oldest samples \n",
    "    def update_memory(self,state,next_state,action,reward,done):\n",
    "        # add most recent trial data to memory\n",
    "        self.memory.append([state,next_state,action,reward,done])\n",
    "\n",
    "        # clip memory if it gets too long    \n",
    "        num_elements = len(self.memory)\n",
    "        if num_elements >= self.memory_length:    \n",
    "            num_delete = num_elements - self.memory_length\n",
    "            self.memory[:num_delete] = []\n",
    "    \n",
    "    # sample from memory and create input / output pairs for regression\n",
    "    def sample_memory(self):\n",
    "        # indices to sample\n",
    "        memory_num = len(self.memory)\n",
    "        sample_nums = np.random.permutation(memory_num)[:self.replay_length]\n",
    "        \n",
    "        # create samples\n",
    "        self.replay_samples = [self.memory[v] for v in sample_nums]\n",
    "    \n",
    "    ##### Q Learning functionality #####\n",
    "    # special function to check done\n",
    "    def check_done(self,done,reward):\n",
    "        if done == True:\n",
    "            reward = -100\n",
    "        return done,reward\n",
    "    \n",
    "    # state normalizer\n",
    "    def state_normalizer(self,states):\n",
    "        states = np.array(states)[np.newaxis,:]\n",
    "        return states\n",
    "    \n",
    "    # choose next action\n",
    "    def choose_action(self,state):\n",
    "        # pick action at random\n",
    "        p = np.random.rand(1)   \n",
    "        action = np.random.randint(self.num_actions)\n",
    "            \n",
    "        # pick action based on exploiting\n",
    "        if len(self.memory) >= self.memory_start:\n",
    "            qs = self.Q(state) \n",
    "            if p > self.explore_val:\n",
    "                action = np.argmax(qs)\n",
    "        return action\n",
    "    \n",
    "    # main training function\n",
    "    def train(self,**kwargs):        \n",
    "        ### start main Q-learning loop ###\n",
    "        for n in range(self.num_episodes): \n",
    "            # pick this episode's starting position - randomly initialize from f_system\n",
    "            state = self.simulator.reset()    \n",
    "            state = self.state_normalizer(state)\n",
    "            total_episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # get out exploit parameter for this episode\n",
    "            if self.explore_val > 0.01:\n",
    "                self.explore_val *= self.explore_decay\n",
    "                    \n",
    "            # run episode\n",
    "            step = 0\n",
    "            while done == False and step < 500:    \n",
    "                # choose next action\n",
    "                action = self.choose_action(state)\n",
    "    \n",
    "                # transition to next state, get associated reward\n",
    "                next_state,reward,done,info = self.simulator.step(action)  \n",
    "                next_state = self.state_normalizer(next_state)\n",
    "                \n",
    "                # update Q function\n",
    "                self.update_Q(state,next_state,action,reward,done)  \n",
    "\n",
    "                # update total reward from this episode\n",
    "                total_episode_reward+=reward\n",
    "                state = copy.deepcopy(next_state)\n",
    "                step+=1\n",
    "                  \n",
    "            # start storing once memory is full \n",
    "            if len(self.memory) >= self.memory_start:\n",
    "                # print out update if verbose set to True\n",
    "                update = 'training episode ' + str(n+1) +  ' of ' + str(self.num_episodes) + ' complete, ' +  ' explore val = ' + str(np.round(self.explore_val,3)) + ', episode reward = ' + str(np.round(total_episode_reward,2)) \n",
    "\n",
    "                self.update_log(self.logname,update + '\\n')\n",
    "                print (update)\n",
    "\n",
    "                update = str(total_episode_reward) + '\\n'\n",
    "                self.update_log(self.reward_logname,update)\n",
    "\n",
    "                ### store this episode's computation time and training reward history\n",
    "                self.training_reward.append(total_episode_reward)\n",
    "\n",
    "                # save latest weights from this episode \n",
    "                update = self.model.get_weights()\n",
    "                self.update_log(self.weight_name,update)\n",
    "            \n",
    "        ### save weights ###\n",
    "        update = 'q-learning algorithm complete'\n",
    "        self.update_log(self.logname,update + '\\n')\n",
    "        print (update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joele119/Desktop/ml_class/Reinforce_hw_2_w_examples/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "\n",
    "# savename\n",
    "savename = 'cartpole_experiment_1'\n",
    "\n",
    "# initialize simulator\n",
    "simulator = gym.make('CartPole-v1') \n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 500\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "\n",
    "# initialize memory\n",
    "replay_length = 100\n",
    "memory_length = 1000\n",
    "\n",
    "# load into instance of learner\n",
    "demo = QLearner(simulator,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,replay_length=replay_length)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [50,50]\n",
    "alpha = 10**(-2)\n",
    "activation = 'relu'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 45 of 500 complete,  explore val = 0.01, episode reward = 39.0\n",
      "training episode 46 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 47 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 48 of 500 complete,  explore val = 0.01, episode reward = 48.0\n",
      "training episode 49 of 500 complete,  explore val = 0.01, episode reward = 145.0\n",
      "training episode 50 of 500 complete,  explore val = 0.01, episode reward = 120.0\n",
      "training episode 51 of 500 complete,  explore val = 0.01, episode reward = 162.0\n",
      "training episode 52 of 500 complete,  explore val = 0.01, episode reward = 154.0\n",
      "training episode 53 of 500 complete,  explore val = 0.01, episode reward = 448.0\n",
      "training episode 54 of 500 complete,  explore val = 0.01, episode reward = 196.0\n",
      "training episode 55 of 500 complete,  explore val = 0.01, episode reward = 21.0\n",
      "training episode 56 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 57 of 500 complete,  explore val = 0.01, episode reward = 16.0\n",
      "training episode 58 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 59 of 500 complete,  explore val = 0.01, episode reward = 120.0\n",
      "training episode 60 of 500 complete,  explore val = 0.01, episode reward = 439.0\n",
      "training episode 61 of 500 complete,  explore val = 0.01, episode reward = 390.0\n",
      "training episode 62 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 63 of 500 complete,  explore val = 0.01, episode reward = 16.0\n",
      "training episode 64 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 65 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 66 of 500 complete,  explore val = 0.01, episode reward = 18.0\n",
      "training episode 67 of 500 complete,  explore val = 0.01, episode reward = 452.0\n",
      "training episode 68 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 69 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 70 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 71 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 72 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 73 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 74 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 75 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 76 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 77 of 500 complete,  explore val = 0.01, episode reward = 15.0\n",
      "training episode 78 of 500 complete,  explore val = 0.01, episode reward = 393.0\n",
      "training episode 79 of 500 complete,  explore val = 0.01, episode reward = 185.0\n",
      "training episode 80 of 500 complete,  explore val = 0.01, episode reward = 109.0\n",
      "training episode 81 of 500 complete,  explore val = 0.01, episode reward = 100.0\n",
      "training episode 82 of 500 complete,  explore val = 0.01, episode reward = 21.0\n",
      "training episode 83 of 500 complete,  explore val = 0.01, episode reward = 18.0\n",
      "training episode 84 of 500 complete,  explore val = 0.01, episode reward = 17.0\n",
      "training episode 85 of 500 complete,  explore val = 0.01, episode reward = 27.0\n",
      "training episode 86 of 500 complete,  explore val = 0.01, episode reward = 18.0\n",
      "training episode 87 of 500 complete,  explore val = 0.01, episode reward = 21.0\n",
      "training episode 88 of 500 complete,  explore val = 0.01, episode reward = 20.0\n",
      "training episode 89 of 500 complete,  explore val = 0.01, episode reward = 20.0\n",
      "training episode 90 of 500 complete,  explore val = 0.01, episode reward = 24.0\n",
      "training episode 91 of 500 complete,  explore val = 0.01, episode reward = 98.0\n",
      "training episode 92 of 500 complete,  explore val = 0.01, episode reward = 133.0\n",
      "training episode 93 of 500 complete,  explore val = 0.01, episode reward = 184.0\n",
      "training episode 94 of 500 complete,  explore val = 0.01, episode reward = 156.0\n",
      "training episode 95 of 500 complete,  explore val = 0.01, episode reward = 259.0\n",
      "training episode 96 of 500 complete,  explore val = 0.01, episode reward = 243.0\n",
      "training episode 97 of 500 complete,  explore val = 0.01, episode reward = 129.0\n",
      "training episode 98 of 500 complete,  explore val = 0.01, episode reward = 178.0\n",
      "training episode 99 of 500 complete,  explore val = 0.01, episode reward = 206.0\n",
      "training episode 100 of 500 complete,  explore val = 0.01, episode reward = 166.0\n",
      "training episode 101 of 500 complete,  explore val = 0.01, episode reward = 162.0\n",
      "training episode 102 of 500 complete,  explore val = 0.01, episode reward = 246.0\n",
      "training episode 103 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 104 of 500 complete,  explore val = 0.01, episode reward = 299.0\n",
      "training episode 105 of 500 complete,  explore val = 0.01, episode reward = 169.0\n",
      "training episode 106 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 107 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 108 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 109 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 110 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 111 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 112 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 113 of 500 complete,  explore val = 0.01, episode reward = 72.0\n",
      "training episode 114 of 500 complete,  explore val = 0.01, episode reward = 66.0\n",
      "training episode 115 of 500 complete,  explore val = 0.01, episode reward = 64.0\n",
      "training episode 116 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 117 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 118 of 500 complete,  explore val = 0.01, episode reward = 123.0\n",
      "training episode 119 of 500 complete,  explore val = 0.01, episode reward = 197.0\n",
      "training episode 120 of 500 complete,  explore val = 0.01, episode reward = 212.0\n",
      "training episode 121 of 500 complete,  explore val = 0.01, episode reward = 251.0\n",
      "training episode 122 of 500 complete,  explore val = 0.01, episode reward = 376.0\n",
      "training episode 123 of 500 complete,  explore val = 0.01, episode reward = 244.0\n",
      "training episode 124 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 125 of 500 complete,  explore val = 0.01, episode reward = 207.0\n",
      "training episode 126 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 127 of 500 complete,  explore val = 0.01, episode reward = 309.0\n",
      "training episode 128 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 129 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 130 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 131 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 132 of 500 complete,  explore val = 0.01, episode reward = 55.0\n",
      "training episode 133 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 134 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 135 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 136 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 137 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 138 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 139 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 140 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 141 of 500 complete,  explore val = 0.01, episode reward = 102.0\n",
      "training episode 142 of 500 complete,  explore val = 0.01, episode reward = 155.0\n",
      "training episode 143 of 500 complete,  explore val = 0.01, episode reward = 322.0\n",
      "training episode 144 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 145 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 146 of 500 complete,  explore val = 0.01, episode reward = 500.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 147 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 148 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 149 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 150 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 151 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 152 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 153 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 154 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 155 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 156 of 500 complete,  explore val = 0.01, episode reward = 66.0\n",
      "training episode 157 of 500 complete,  explore val = 0.01, episode reward = 8.0\n",
      "training episode 158 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 159 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 160 of 500 complete,  explore val = 0.01, episode reward = 8.0\n",
      "training episode 161 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 162 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 163 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 164 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 165 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 166 of 500 complete,  explore val = 0.01, episode reward = 198.0\n",
      "training episode 167 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 168 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 169 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 170 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 171 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 172 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 173 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 174 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 175 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 176 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 177 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 178 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 179 of 500 complete,  explore val = 0.01, episode reward = 161.0\n",
      "training episode 180 of 500 complete,  explore val = 0.01, episode reward = 91.0\n",
      "training episode 181 of 500 complete,  explore val = 0.01, episode reward = 92.0\n",
      "training episode 182 of 500 complete,  explore val = 0.01, episode reward = 110.0\n",
      "training episode 183 of 500 complete,  explore val = 0.01, episode reward = 174.0\n",
      "training episode 184 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 185 of 500 complete,  explore val = 0.01, episode reward = 121.0\n",
      "training episode 186 of 500 complete,  explore val = 0.01, episode reward = 64.0\n",
      "training episode 187 of 500 complete,  explore val = 0.01, episode reward = 90.0\n",
      "training episode 188 of 500 complete,  explore val = 0.01, episode reward = 91.0\n",
      "training episode 189 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 190 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 191 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 192 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 193 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 194 of 500 complete,  explore val = 0.01, episode reward = 15.0\n",
      "training episode 195 of 500 complete,  explore val = 0.01, episode reward = 183.0\n",
      "training episode 196 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 197 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 198 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 199 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 200 of 500 complete,  explore val = 0.01, episode reward = 11.0\n",
      "training episode 201 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 202 of 500 complete,  explore val = 0.01, episode reward = 40.0\n",
      "training episode 203 of 500 complete,  explore val = 0.01, episode reward = 234.0\n",
      "training episode 204 of 500 complete,  explore val = 0.01, episode reward = 213.0\n",
      "training episode 205 of 500 complete,  explore val = 0.01, episode reward = 205.0\n",
      "training episode 206 of 500 complete,  explore val = 0.01, episode reward = 183.0\n",
      "training episode 207 of 500 complete,  explore val = 0.01, episode reward = 148.0\n",
      "training episode 208 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 209 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 210 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 211 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 212 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 213 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 214 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 215 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 216 of 500 complete,  explore val = 0.01, episode reward = 9.0\n",
      "training episode 217 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 218 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 219 of 500 complete,  explore val = 0.01, episode reward = 167.0\n",
      "training episode 220 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 221 of 500 complete,  explore val = 0.01, episode reward = 16.0\n",
      "training episode 222 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 223 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 224 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 225 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 226 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 227 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 228 of 500 complete,  explore val = 0.01, episode reward = 10.0\n",
      "training episode 229 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 230 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 231 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 232 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 233 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 234 of 500 complete,  explore val = 0.01, episode reward = 13.0\n",
      "training episode 235 of 500 complete,  explore val = 0.01, episode reward = 12.0\n",
      "training episode 236 of 500 complete,  explore val = 0.01, episode reward = 15.0\n",
      "training episode 237 of 500 complete,  explore val = 0.01, episode reward = 15.0\n",
      "training episode 238 of 500 complete,  explore val = 0.01, episode reward = 14.0\n",
      "training episode 239 of 500 complete,  explore val = 0.01, episode reward = 16.0\n",
      "training episode 240 of 500 complete,  explore val = 0.01, episode reward = 20.0\n",
      "training episode 241 of 500 complete,  explore val = 0.01, episode reward = 19.0\n",
      "training episode 242 of 500 complete,  explore val = 0.01, episode reward = 98.0\n",
      "training episode 243 of 500 complete,  explore val = 0.01, episode reward = 289.0\n",
      "training episode 244 of 500 complete,  explore val = 0.01, episode reward = 189.0\n",
      "training episode 245 of 500 complete,  explore val = 0.01, episode reward = 220.0\n",
      "training episode 246 of 500 complete,  explore val = 0.01, episode reward = 500.0\n",
      "training episode 247 of 500 complete,  explore val = 0.01, episode reward = 313.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 248 of 500 complete,  explore val = 0.01, episode reward = 256.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf8c7d3d2c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-7ab23a00c559>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;31m# update Q function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;31m# update total reward from this episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7ab23a00c559>\u001b[0m in \u001b[0;36mupdate_Q\u001b[0;34m(self, state, next_state, action, reward, done)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# clamp all other models to their current values for this input/output pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mq_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mq_update\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mq_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1064\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1329\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m       \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m       \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m       \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m       \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_reward_history(logname,**kwargs):\n",
    "    # load in total episode reward history\n",
    "    data = np.loadtxt(logname)\n",
    "    ave = [data[v] for v in range(100)]\n",
    "    \n",
    "    for i in range(0,np.size(data)-100):\n",
    "        m = np.mean(data[i:i+100])\n",
    "        ave.append(m)\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(figsize = (12,8))\n",
    "    ax1 = fig.add_subplot(2,1,1)\n",
    "    ax2 = fig.add_subplot(2,1,2)\n",
    "\n",
    "    # plot total reward history\n",
    "    start = 0\n",
    "    if 'start' in kwargs:\n",
    "        start = kwargs['start']\n",
    "    ax1.plot(data[start:])\n",
    "    ax1.set_xlabel('episode',labelpad = 8,fontsize = 13)\n",
    "    ax1.set_ylabel('total reward',fontsize = 13)\n",
    "    \n",
    "    ax2.plot(ave[start:],linewidth=3)\n",
    "    ax2.set_xlabel('episode',labelpad = 8,fontsize = 13)\n",
    "    ax2.set_ylabel('ave total reward',fontsize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plot_reward_history(reward_logname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
