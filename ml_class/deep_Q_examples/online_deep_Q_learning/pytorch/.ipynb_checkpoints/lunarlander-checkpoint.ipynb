{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "PyTorch does not the same abstract API Keras does for optimization, in particular\n",
    "while it does contain an array of advanced first order methods like e.g., RMSprop\n",
    "one needs to construct one's own looping structure in order to employ it properly. \n",
    "Below the class My_Opt does just this.  \n",
    "'''\n",
    "class My_Opt:\n",
    "    def __init__(self,model,cost):\n",
    "        self.model = model\n",
    "        self.cost = cost\n",
    "        learning_rate = 10**(-2)\n",
    "        self.optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # record history\n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        \n",
    "    def fit(self,x,y,max_its,lr):\n",
    "        # update learning rate\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        for t in range(max_its):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = self.model(x.float())\n",
    "\n",
    "            # Compute and print loss.\n",
    "            cost_val = self.cost(y_pred, y.float())\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if len(self.cost_history) == 0 or t > 0:\n",
    "                # store weight and cost history\n",
    "                self.cost_history.append(cost_val.data.item())\n",
    "                self.weight_history.append(self.optimizer.param_groups[0]['params'])\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "            cost_val.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # collect last input / weights\n",
    "        y_pred = self.model(x.float())\n",
    "        cost_val = self.cost(y_pred, y.float())\n",
    "        self.cost_history.append(cost_val.data.item())\n",
    "        self.weight_history.append(self.optimizer.param_groups[0]['params'])\n",
    "\n",
    "class QLearner():\n",
    "    # load in simulator, initialize global variables\n",
    "    def __init__(self,simulator,savename,**kwargs):\n",
    "        # make simulator global\n",
    "        self.simulator = simulator\n",
    "        \n",
    "        # Q learn params\n",
    "        self.explore_val = 1\n",
    "        self.explore_decay = 0.99\n",
    "        self.num_episodes = 500        \n",
    "        self.gamma = 1\n",
    "        \n",
    "        if \"gamma\" in kwargs:   \n",
    "            self.gamma = args['gamma']\n",
    "        if 'explore_val' in kwargs:\n",
    "            self.explore_val = kwargs['explore_val']\n",
    "        if 'explore_decay' in kwargs:\n",
    "            self.explore_decay = kwargs['explore_decay']\n",
    "        if 'num_episodes' in kwargs:\n",
    "            self.num_episodes = kwargs['num_episodes']\n",
    "            \n",
    "        # other training variables\n",
    "        self.num_actions = self.simulator.action_space.n\n",
    "        state = self.simulator.reset()    \n",
    "        self.state_dim = np.size(state)\n",
    "        self.training_reward = []\n",
    "        \n",
    "        # setup memory params\n",
    "        self.memory_length = 1000    # length of memory replay\n",
    "        self.replay_length = 100     # length of replay sample\n",
    "        self.memory_start = 1000\n",
    "        self.memory = []\n",
    "        if 'memory_length' in kwargs:\n",
    "            self.memory_length = kwargs['memory_length']\n",
    "        if 'replay_length' in kwargs:\n",
    "            self.replay_length = kwargs['replay_length']\n",
    "        if 'memory_start' in kwargs:\n",
    "            self.memory_start = kwargs['memory_start']\n",
    "            \n",
    "        ### initialize logs ###\n",
    "        # create text file for training log\n",
    "        self.logname = 'training_logs/' + savename + '.txt'\n",
    "        self.reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "        self.weight_name = 'saved_model_weights/' + savename + '.pkl' \n",
    "        self.model_name = 'models/' + savename + '.json'\n",
    "\n",
    "        self.init_log(self.logname)\n",
    "        self.init_log(self.reward_logname)\n",
    "        self.init_log(self.weight_name)\n",
    "        self.init_log(self.model_name)\n",
    "     \n",
    "    ##### logging functions #####\n",
    "    def init_log(self,logname):\n",
    "        # delete log if old version exists\n",
    "        if os.path.exists(logname): \n",
    "            os.remove(logname)\n",
    "            \n",
    "    def update_log(self,logname,update):\n",
    "        if type(update) == str:\n",
    "            logfile = open(logname, \"a\")\n",
    "            logfile.write(update)\n",
    "            logfile.close() \n",
    "        else:\n",
    "            weights = []\n",
    "            if os.path.exists(logname):\n",
    "                with open(logname,'rb') as rfp: \n",
    "                    weights = pickle.load(rfp)\n",
    "            weights.append(update)\n",
    "\n",
    "            with open(logname,'wb') as wfp:\n",
    "                pickle.dump(weights, wfp)\n",
    "    \n",
    "    ##### functions for creating / updating Q #####\n",
    "    def initialize_Q(self,**kwargs):\n",
    "        # default parameters for network\n",
    "        layer_sizes = [10,10]      # two hidden layers, 10 units each, by default\n",
    "        activation = 'relu'\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        if 'activation' in kwargs:\n",
    "            activation = kwargs['activation']\n",
    "\n",
    "        # default parameters for optimizer - reset by hand\n",
    "        loss = 'mse'\n",
    "        self.lr = 10**(-2)\n",
    "        if 'alpha' in kwargs:\n",
    "            self.lr = kwargs['alpha']\n",
    "\n",
    "        # input / output sizes of network\n",
    "        input_dim = self.state_dim\n",
    "        output_dim = self.num_actions\n",
    "\n",
    "        # Use the nn package to define our model and loss function\n",
    "        self.model = torch.nn.Sequential()\n",
    "\n",
    "        # add input layer\n",
    "        self.model.add_module('linear ' + str(0),torch.nn.Linear(input_dim,layer_sizes[0]))\n",
    "        if activation == 'relu':\n",
    "            self.model.add_module('activation ' + str(0),torch.nn.ReLU())\n",
    "        if activation == 'tanh':\n",
    "            self.model.add_module('activation ' + str(0),torch.nn.Tanh())\n",
    "\n",
    "        # add hidden layers\n",
    "        for i in range(1,len(layer_sizes)):\n",
    "            U = layer_sizes[i-1]\n",
    "            V = layer_sizes[i]\n",
    "            self.model.add_module('linear ' + str(i),torch.nn.Linear(U,V))\n",
    "            if activation == 'relu':\n",
    "                self.model.add_module('activation ' + str(i),torch.nn.ReLU())\n",
    "            if activation == 'tanh':\n",
    "                self.model.add_module('activation ' + str(i),torch.nn.Tanh())\n",
    "\n",
    "        # add output layer\n",
    "        self.model.add_module('linear ' + str(len(layer_sizes)),torch.nn.Linear(layer_sizes[-1], output_dim))\n",
    "\n",
    "        # define cost function\n",
    "        self.cost = torch.nn.MSELoss(size_average=True)\n",
    "        \n",
    "        # setup optimizer - using hand-made optimizer class + RMSprop\n",
    "        self.opt = My_Opt(self.model,self.cost)\n",
    "\n",
    "        # initialize Q\n",
    "        self.Q = self.opt.model.forward\n",
    "\n",
    "    # update Q function\n",
    "    def update_Q(self,state,next_state,action,reward,done):\n",
    "        # add newest sample to queue\n",
    "        self.update_memory(state,next_state,action,reward,done)\n",
    "        \n",
    "        # only update Q if sufficient memory has been collected\n",
    "        if len(self.memory) < self.memory_start:\n",
    "            return\n",
    "\n",
    "        # update memory sample\n",
    "        self.sample_memory()\n",
    "        \n",
    "        # generate q_values based on most recent Q\n",
    "        q_vals = []\n",
    "        states = []\n",
    "        for i in range(len(self.replay_samples)):    \n",
    "            # get sample\n",
    "            sample = self.replay_samples[i]\n",
    "            \n",
    "            # strip sample for parts\n",
    "            state = sample[0]\n",
    "            next_state = sample[1]\n",
    "            action = sample[2]\n",
    "            reward = sample[3]\n",
    "            done = sample[4]\n",
    "                            \n",
    "            ### for cartpole only - check if done, and alter reward to improve learning ###\n",
    "            #done,reward = self.check_done(done,reward)\n",
    "\n",
    "            # compute and store q value\n",
    "            q = reward \n",
    "            if done == False:\n",
    "                next_state = Variable(torch.from_numpy(next_state),requires_grad=False)        \n",
    "                qs = self.Q(next_state.float()).detach().numpy()\n",
    "                q += self.gamma*np.max(qs)\n",
    "            \n",
    "            # clamp all other models to their current values for this input/output pair\n",
    "            state = Variable(torch.from_numpy(state),requires_grad=False)        \n",
    "            q_update = self.Q(state.float()).detach().numpy().flatten()\n",
    "            q_update[action] = q\n",
    "            q_vals.append(q_update)\n",
    "            states.append(state.numpy().flatten())\n",
    "        \n",
    "        # convert lists to numpy arrays for regressor\n",
    "        states = np.array(states).T\n",
    "        states = torch.from_numpy(states)\n",
    "        s_in = Variable(states,requires_grad = False)\n",
    "        q_vals = np.array(q_vals).T\n",
    "        q_vals = torch.from_numpy(q_vals)\n",
    "        q_vals = Variable(q_vals,requires_grad = False)\n",
    "        \n",
    "        # take descent step\n",
    "        self.opt.fit(s_in.t(),q_vals.t(),max_its = 1,lr=self.lr)\n",
    "\n",
    "        # update Q based on regressor updates\n",
    "        self.Q = self.opt.model.forward\n",
    "        \n",
    "    ##### functions for adjusting replay memory #####\n",
    "    # update memory - add sample to list, remove oldest samples \n",
    "    def update_memory(self,state,next_state,action,reward,done):\n",
    "        # add most recent trial data to memory\n",
    "        self.memory.append([state,next_state,action,reward,done])\n",
    "\n",
    "        # clip memory if it gets too long    \n",
    "        num_elements = len(self.memory)\n",
    "        if num_elements >= self.memory_length:    \n",
    "            num_delete = num_elements - self.memory_length\n",
    "            self.memory[:num_delete] = []\n",
    "    \n",
    "    # sample from memory and create input / output pairs for regression\n",
    "    def sample_memory(self):\n",
    "        # indices to sample\n",
    "        memory_num = len(self.memory)\n",
    "        sample_nums = np.random.permutation(memory_num)[:self.replay_length]\n",
    "        \n",
    "        # create samples\n",
    "        self.replay_samples = [self.memory[v] for v in sample_nums]\n",
    "    \n",
    "    ##### Q Learning functionality #####\n",
    "    # special function to check done\n",
    "    def check_done(self,done,reward):\n",
    "        if done == True:\n",
    "            reward = -100\n",
    "        return done,reward\n",
    "    \n",
    "    # state normalizer\n",
    "    def state_normalizer(self,states):\n",
    "        states = np.array(states)[np.newaxis,:]\n",
    "        return states\n",
    "    \n",
    "    # choose next action\n",
    "    def choose_action(self,state):\n",
    "        # pick action at random\n",
    "        p = np.random.rand(1)   \n",
    "        action = np.random.randint(self.num_actions)\n",
    "            \n",
    "        # pick action based on exploiting - after memory full\n",
    "        if len(self.memory) >= self.memory_start:\n",
    "            state = Variable(torch.from_numpy(state),requires_grad=False)        \n",
    "            qs = self.Q(state.float()).detach().numpy()\n",
    "            if p > self.explore_val:\n",
    "                action = np.argmax(qs)\n",
    "        return action\n",
    "    \n",
    "    # main training function\n",
    "    def train(self,**kwargs):\n",
    "        \n",
    "        ### start main Q-learning loop ###\n",
    "        for n in range(self.num_episodes): \n",
    "            # pick this episode's starting position - randomly initialize from f_system\n",
    "            state = self.simulator.reset()    \n",
    "            state = self.state_normalizer(state)\n",
    "            total_episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # get out exploit parameter for this episode\n",
    "            if self.explore_val > 0.01:\n",
    "                self.explore_val *= self.explore_decay\n",
    "                    \n",
    "            # run episode\n",
    "            while done == False:    \n",
    "                # choose next action\n",
    "                action = self.choose_action(state)\n",
    "    \n",
    "                # transition to next state, get associated reward\n",
    "                next_state,reward,done,info = self.simulator.step(action)  \n",
    "                next_state = self.state_normalizer(next_state)\n",
    "                \n",
    "                # update Q function\n",
    "                self.update_Q(state,next_state,action,reward,done)  \n",
    "\n",
    "                # update total reward from this episode\n",
    "                total_episode_reward+=reward\n",
    "                state = copy.deepcopy(next_state)\n",
    "                  \n",
    "            # start storing once memory is full \n",
    "            if len(self.memory) >= self.memory_start:\n",
    "                # print out update if verbose set to True\n",
    "                update = 'training episode ' + str(n+1) +  ' of ' + str(self.num_episodes) + ' complete, ' +  ' explore val = ' + str(np.round(self.explore_val,3)) + ', episode reward = ' + str(np.round(total_episode_reward,2)) \n",
    "\n",
    "                self.update_log(self.logname,update + '\\n')\n",
    "                print (update)\n",
    "\n",
    "                update = str(total_episode_reward) + '\\n'\n",
    "                self.update_log(self.reward_logname,update)\n",
    "\n",
    "                ### store this episode's computation time and training reward history\n",
    "                self.training_reward.append(total_episode_reward)\n",
    "            \n",
    "                # save latest weights from this episode \n",
    "                update = self.opt.weight_history[-1]\n",
    "                self.update_log(self.weight_name,update)\n",
    "                            \n",
    "        ### save weights ###\n",
    "        update = 'q-learning algorithm complete'\n",
    "        self.update_log(self.logname,update + '\\n')\n",
    "        print (update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "\n",
    "# savename\n",
    "savename = 'lunarlander_experiment_1'\n",
    "\n",
    "# initialize simulator\n",
    "simulator = gym.make('LunarLander-v2') \n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 10000\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "\n",
    "# initialize memory\n",
    "replay_length = 100\n",
    "memory_length = 1000\n",
    "\n",
    "# load into instance of learner\n",
    "demo = QLearner(simulator,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,replay_length=replay_length)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [50,50,50]\n",
    "alpha = 10**(-2)\n",
    "activation = 'tanh'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 12 of 10000 complete,  explore val = 0.01, episode reward = -155.38\n",
      "training episode 13 of 10000 complete,  explore val = 0.01, episode reward = -717.72\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3199fc223f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-54c99bae5d67>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# update Q function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0;31m# update total reward from this episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-54c99bae5d67>\u001b[0m in \u001b[0;36mupdate_Q\u001b[0;34m(self, state, next_state, action, reward, done)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0mq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_reward_history(logname,**kwargs):\n",
    "    # load in total episode reward history\n",
    "    data = np.loadtxt(logname)\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(figsize = (12,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    # plot total reward history\n",
    "    start = 0\n",
    "    if 'start' in kwargs:\n",
    "        start = kwargs['start']\n",
    "    ax.plot(data[start:])\n",
    "    ax.set_xlabel('episode')\n",
    "    ax.set_ylabel('total reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plot_reward_history(reward_logname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# produce animation of validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in weights\n",
    "infile = demo.weight_name\n",
    "with open(infile, 'rb') as in_strm:\n",
    "    datastruct = pickle.load(in_strm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(datastruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
