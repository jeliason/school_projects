{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effects of Randomness in training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Joel Eliason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is a joint project for both Optimization in Deep Learning and Deep Learning from Scratch. Therefore, the notebook submitted for the two projects is the same (this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many scientific and commerical applications (including, of course, machine learning), we strive for high precision and accuracy. In fact, a common lay explanation of training neural networks is that, by tuning the weights of a nested composition of functions \"just right\", we are actually able to train a network to compute any function we desire (i.e. Turing completeness). However, many scientific studies have shown that inserting or building in randomness or lower precision into a neural network often results in comparable performance (and in a few cases, performance that exceeds that of its higher precision counterpart). This is apparent, actually, from the widespread use of stochastic gradient descent. This form of gradient descent operates by calculating the gradient of a small subset of samples (often called a batch or minibatch), instead of calculating the gradient on the whole sample set. The fundamental concept motivating this idea (and the ones we will examine below) is that adding noise to training sometimes allows the optimizer to \"jump\" out of shallower local minima in order to travel down steeper, more global minima. What's more, an examination of the effects of randomness in NN training is useful for environments in which lower precision is more of a necessity (for example, neuromorphic computation or embedded systems) or the hardware environment in which training is being performed is more susceptible to noise. Therefore, this project is motivated by the need to understand how generic neural networks will perform when being trained under more adversarial, noisy or memory-constrained conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, in this project, I will be performing 8 short experiments that subject neural network training to different forms of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will utilize the PyTorch modeling language for all experiments. To run this code, you will need torch and torchvision installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are utility functions. The first allows us to see the images in the data set, while the second one outputs a % accuracy of a trained network on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "def accuracy(net):\n",
    "    outputs = net(Variable(images1))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neuromorophic computation or embedded systems will most likely utilize smaller neural networks due to memory constraints, in this project I used a 6-layer ConvNet built in PyTorch (in fact, the net I use is a direct clone of the one used in the official tutorial on using PyTorch for deep learning). The 6 layers can be seen below. The first layer is a convolutional layer, in which each \"neuron\" acts as a trainable filter for the input data. Following that is a MaxPool layer, in which the max value in each 2x2 square of the input is taken as the output for that square. The last 3 layers are linear layers, which are fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2_bn(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is my training function. This function takes a 'switch' argument that tells it which experiment to run. As can be seen, there are 8 different experiments that I will be running. Additionally, I will be training for 2 epochs per experiment (2 passes over the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(switch):\n",
    "    count_jump=0\n",
    "    if switch=='bias':\n",
    "        biases=[];\n",
    "        for p in net.parameters():\n",
    "            biases.append(torch.from_numpy(np.random.uniform(-scale,scale,size=p.data.size())).float())\n",
    "    elif switch=='binarizeweights':\n",
    "        clips=[]\n",
    "        for p in net.parameters():\n",
    "            clips.append(f*np.std(p.data.numpy()))\n",
    "    for epoch in range(2):  # loop over the dataset twice\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if switch=='none':\n",
    "                loss.backward()\n",
    "            elif switch=='rgrad':\n",
    "                loss.backward(scale*torch.randn(1,1))\n",
    "            elif switch=='addnoise':\n",
    "                loss.backward()\n",
    "                for p in net.parameters():\n",
    "                    p.grad.data.add_(torch.from_numpy(np.random.normal(0,variance,size=p.data.shape)).float())\n",
    "            elif switch=='mulnoise':\n",
    "                loss.backward()\n",
    "                for p in net.parameters():\n",
    "                    p.grad.data.mul_(torch.from_numpy(np.random.normal(1,variance,size=p.data.shape)).float())\n",
    "            elif switch=='bias':\n",
    "                loss.backward()\n",
    "                count=0\n",
    "                for p in net.parameters():\n",
    "                    p.data.add_(biases[count])\n",
    "                    count+=1\n",
    "            elif switch=='jump':\n",
    "                to_jump=np.random.choice([True, False], size=(1,), p=[1./frequency, np.true_divide((frequency-1),frequency)])\n",
    "                loss.backward()\n",
    "                if to_jump:\n",
    "                    count_jump+=1\n",
    "                    print \"Jump here\"\n",
    "                    for p in net.parameters():\n",
    "                        p.data.add_(torch.from_numpy(np.random.normal(0,variance,size=p.data.shape)).float())\n",
    "            elif switch=='randomizeweights':\n",
    "                for p in net.parameters():\n",
    "                    bools=np.random.choice([1,0],size=p.data.shape,p=[1./frequency, np.true_divide((frequency-1),frequency)])\n",
    "                    bools=torch.from_numpy(bools)\n",
    "                    p.data[bools==1]=0\n",
    "                    rands=torch.from_numpy(np.random.normal(0,variance,size=p.data.shape)).float()\n",
    "                    rands[bools==0]=0\n",
    "                    p.data.add_(rands)\n",
    "                loss.backward()\n",
    "            elif switch=='binarizeweights':\n",
    "                max_weights=[]\n",
    "                weight_holder=[]\n",
    "                count=0\n",
    "                for p in net.parameters():\n",
    "                    max_weights.append(float(np.max(p.data.numpy())))\n",
    "                    weight_holder.append(p.data.clone())\n",
    "                    p.data[p.data<0]=-max_weights[count]\n",
    "                    p.data[p.data>0]=max_weights[count]\n",
    "                    count+=1\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                count=0\n",
    "                for p in net.parameters():\n",
    "                    p.data=weight_holder[count]\n",
    "                    p.data.clamp(-clips[count],clips[count])\n",
    "                    count+=1\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    if switch=='jump':\n",
    "        print \"Number of jumps = \"+str(count_jump)\n",
    "    elif switch=='binarizeweights':\n",
    "        for p in net.parameters():\n",
    "            p.data[p.data<0]=-1\n",
    "            p.data[p.data>0]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalized range [-1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some of the images that our network will be seeing during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dog truck   dog  frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2dd3e9a250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images1, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images1))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. No stochasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first experiment, we establish a baseline: here, we have done training without any weight or gradient distortions or perturbations. As we can see, for being such a small ConvNet and for only training for 2 epochs, we get quite good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.161\n",
      "[1,  4000] loss: 1.818\n",
      "[1,  6000] loss: 1.655\n",
      "[1,  8000] loss: 1.558\n",
      "[1, 10000] loss: 1.512\n",
      "[1, 12000] loss: 1.453\n",
      "[2,  2000] loss: 1.389\n",
      "[2,  4000] loss: 1.353\n",
      "[2,  6000] loss: 1.357\n",
      "[2,  8000] loss: 1.320\n",
      "[2, 10000] loss: 1.301\n",
      "[2, 12000] loss: 1.272\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 55 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, I test the hypothesis that we don't need to retain any information about the cost function at all when starting backpropagation (this hypothesis is, of course, ludicrous, since without cost function gradient information, we are just doing a random walk around the cost function). Nevertheless, this helps us establish a baseline of \"maximum randomness\" to compare against in further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "scale=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.307\n",
      "[1,  4000] loss: 2.304\n",
      "[1,  6000] loss: 2.306\n",
      "[1,  8000] loss: 2.305\n",
      "[1, 10000] loss: 2.305\n",
      "[1, 12000] loss: 2.304\n",
      "[2,  2000] loss: 2.307\n",
      "[2,  4000] loss: 2.307\n",
      "[2,  6000] loss: 2.306\n",
      "[2,  8000] loss: 2.306\n",
      "[2, 10000] loss: 2.308\n",
      "[2, 12000] loss: 2.307\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train('rgrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive and multiplicative noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Adding white noise to the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, I implement a common practice in training neural networks: that of adding noise during training to improve training. The intuition behind this is the same as in stochastic gradient descent: a little bit of randomness can allow us to escape shallower local minima, in favor of (hopefully) deeper minima.  In this particular implementation, I add noise to the gradients (rather than the weights themselves). This experiment was inspired by Ref. [1], which implements the noise according to a particular schedule. Here, however, I simply add a small bit of normally-distributed noise to the gradient of the cost wrt every weight during each minibatch. As it turns out, with a network this small and low numbers of epochs, the impact of noise is very slight (in fact, in this case has no impact on the classification error). Additionally, the variance of this noise is another knob to tune here: too high of variance leads to divergence in the cost function, while too low will have no impact on training at all. During my experiments, I could not find a noise variance that had a significantly positive impact on training, which matches my intuition. However, variance that yields even quite large (relatively speaking) noise signals does not have that detrimental of an impact (see below: variance = 0.1 yields a 47% accuracy, not much lower than the totally noise-free training above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "variance=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.205\n",
      "[1,  4000] loss: 1.831\n",
      "[1,  6000] loss: 1.716\n",
      "[1,  8000] loss: 1.633\n",
      "[1, 10000] loss: 1.586\n",
      "[1, 12000] loss: 1.567\n",
      "[2,  2000] loss: 1.535\n",
      "[2,  4000] loss: 1.510\n",
      "[2,  6000] loss: 1.501\n",
      "[2,  8000] loss: 1.504\n",
      "[2, 10000] loss: 1.493\n",
      "[2, 12000] loss: 1.495\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train('addnoise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 47 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Multiplying the gradient by white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment is a slightly different tack on the previous one. Here, instead of adding noise to the gradients, I multiply each gradient during each minibatch by a small bit of normally distributed noise (with mean 1). This is, in theory, very similar to adding noise to the gradients. However, here we can use a much larger variance than in the additive noise case (try it! set both additive and multiplicative variance to 0.1). It's not intuitively clear to me why this should be the case, so further experiments could be designed to probe this case further. Again, in this experiment, the variance is the knob to tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "variance=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.196\n",
      "[1,  4000] loss: 1.894\n",
      "[1,  6000] loss: 1.707\n",
      "[1,  8000] loss: 1.592\n",
      "[1, 10000] loss: 1.551\n",
      "[1, 12000] loss: 1.478\n",
      "[2,  2000] loss: 1.432\n",
      "[2,  4000] loss: 1.420\n",
      "[2,  6000] loss: 1.398\n",
      "[2,  8000] loss: 1.387\n",
      "[2, 10000] loss: 1.365\n",
      "[2, 12000] loss: 1.357\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train('mulnoise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 52 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Constant bias added to weights after every backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment was a failed attempt at feedback alignment (Ref.[2]), in which error information is backpropagated along random channels (instead of along the channels that connect each neuron in each layer). However, tthe time cost with which I was having to break down the PyTorch code to implement this was becoming prohibitive, so I simply ended up adding a constant bias to each weight after every backward pass. This experiment has no theoretical basis for actually working, since adding a constant (uniformly distributed) bias will always steer the weights along the cost function in a counterproductive direction, unless that bias happens to be in the direction of steepest descent (which is exponentially unlikely in such a high-dimensional space). The knob to tune here is the magnitude of the bias (the scale parameter below). Gradient descent is unable to compensate for a bias that is larger than the learning rate - thus, the key to compensating for such a bias is to have an adaptive learning rate, in which the network \"realizes\" that it is being biased in a counterproductive direction and then compensates by updating the learning rate to be larger than the magnitude of the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "scale=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 11.647\n",
      "[1,  4000] loss: nan\n",
      "[1,  6000] loss: nan\n",
      "[1,  8000] loss: nan\n",
      "[1, 10000] loss: nan\n",
      "[1, 12000] loss: nan\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "[2, 12000] loss: nan\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train('bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Stochastic jump to new area of cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many minima that are approximately equally as good as one another in the high-dimensional cost function space. This experiment was designed to explore the cost function space more thoroughly: at random times, a random jump is administed to all the weights in the net, \"jumping\" it to a completely different area of the cost function that would have otherwise been unexplored. The frequency of jumps is the varying parameter here. If there are too many jumps, the cost function doesn't get too far beyond taking a few optimizing steps before jumping to a new location. Here, jump times are assigned randomly - however, one could imagine a better version of this \"cost function exploration\" idea as deciding to jump when the magnitude of the gradient drops below a certain threshold (indicating a shallowing out of the cost function), but the cost function itself is still above a different threshold. This would, in essence, allow the net to escape saddle points, which often trap optimizers. Additionally, I tried increasing the learning rate, so that optimization could proceed more quickly once a jump had been made. However,  I was not able to show that this had a positive impact, for any frequency. All in all, stochastic jumping, as implemented here, reduced neural network training to \"maximum randomness\". When testing, this frequency should not be much greater than about 20000 (otherwise jumps never occur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # could try increasing the learning rate in this experiment\n",
    "frequency=20000 # this is actually inverse of frequency, so the higher the frequency, the less jumps\n",
    "variance = 1 # if variance = 0.1, training does not get totally upended (since weights don't move as far) as it does with variance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.160\n",
      "[1,  4000] loss: 1.848\n",
      "[1,  6000] loss: 1.717\n",
      "[1,  8000] loss: 1.586\n",
      "[1, 10000] loss: 1.524\n",
      "Jump here\n",
      "[1, 12000] loss: nan\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "[2, 12000] loss: nan\n",
      "Finished Training\n",
      "Number of jumps = 1\n"
     ]
    }
   ],
   "source": [
    "train('jump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Binarize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concept was the most interesting one that I examined. In this experiment, I restricted the values of the weights and activations on the forward and backward pass to be 1 and -1. Such a weight projection is called \"low-precision\", since these weights only require 1 bit of precision to be stored. Such low demands on memory are highly desirable for embedded systems and neuromorphic computing, which are often strapped for memory and power consumption. At least a few papers have shown that binary neural networks (BNNs) perform similarly to higher precision neural networks (Refs. [3], [4]). In my experiment, I was not able to achieve any sort of good performance, even after implementing the exact algorithm from these references. My opinion on this is that my ConvNet is much smaller than in those references, and thus may not be able to encode very complex representations using only binary weights. Of course, further experimentation with various architectures would be needed to really flesh out the utility of binarization for small neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "f=0.5 # scales the clipping parameter, should be left at f = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.274\n",
      "[1,  4000] loss: 2.311\n",
      "[1,  6000] loss: 2.314\n",
      "[1,  8000] loss: 2.312\n",
      "[1, 10000] loss: 2.311\n",
      "[1, 12000] loss: 2.311\n",
      "[2,  2000] loss: 2.312\n",
      "[2,  4000] loss: 2.314\n",
      "[2,  6000] loss: 2.311\n",
      "[2,  8000] loss: 2.312\n",
      "[2, 10000] loss: 2.311\n",
      "[2, 12000] loss: 2.312\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train('binarizeweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Randomize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last experiment, I devised a concept somewhat reminiscent of the regularization technique of Dropout. Dropout is a method to induce robustness and encourage finding global minima, in which subsets of weights are \"frozen\" (i.e. not used in training) on particular minibatches. In contrast, my technique, instead of freezing the weights, simply reassigns them to normally-distributed numbers. This is a similar technique to the stochastic jump experiment examined above, in which all the weights are randomly perturbed. This technique differs in that only a very small subset of the weights are reassigned during each minibatch. Again, the \"frequency\" and variance of the randomly reassigned number are the knobs to tune in this example. In running this experiment, changing the variance below 0.1 did not have an effect on training while the frequency was at or above 100000. For frequencies less than this, weight randomization was influenced by both variance and frequency - a lower \"frequency\" requires a lower variance of noise in order to still train effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "frequency=10000 # this is actually inverse of frequency, so the higher the frequency, the less random weight switching\n",
    "variance=0.1 # don't change it above 0.5 - will cause divergence in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.224\n",
      "[1,  4000] loss: 1.856\n",
      "[1,  6000] loss: 1.722\n",
      "[1,  8000] loss: 1.651\n",
      "[1, 10000] loss: 1.630\n",
      "[1, 12000] loss: 1.606\n",
      "[2,  2000] loss: 1.577\n",
      "[2,  4000] loss: 1.583\n",
      "[2,  6000] loss: 1.552\n",
      "[2,  8000] loss: 1.566\n",
      "[2, 10000] loss: 1.563\n",
      "[2, 12000] loss: 1.534\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train('randomizeweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 44 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focused on the effects of random perturbations to weights or gradients during training of neural networks. In particular, I used the heuristic of injecting randomness to the training process to encourage the escaping of \"shallow minima\". Many of the above experiments showed similar results: for example, randomization of the weights, additive noise and multiplicative noise all did not significantly alter the training process, as long as the noise variances or the application of the noisiness (as in weight randomization) were not too high. In some of these experiments (stochastic jumping and weight randomization), two parameters were present. Performing analyses of training in two parameters takes much more time, since there are more combinations of parameters to test and sometimes subtle interactions between parameters. Therefore, due to time constraints, I was not able to flesh out more precise relationships between the parameters and training in these two experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I focused on the effects of randomness in training neural networks. In particular, I focused on these effects for small neural networks with low training times. Noise did not have a significant positive impact in my experiments, as it has in previous works. However, if noise was small, the lower precision induced by noise did not significantly impact the training of small neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further work in this direction would include examining more closely the difference between multiplicative noise and additive noise, and in particular, why one seems to have a more significant impact than the other. In addition, I would like to further investigate weight binarization in small neural networks, whether or not it is possible in small nets, and to what degree. Lastly, it will be important to get sharper bounds on allowable noise in small neural net training, as understanding the precise relationship between noise and neural network training will be paramount in neuromorphic computing and embedded systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach: “Adding Gradient Noise Improves Learning for Very Deep Networks”, 2015; arXiv:1511.06807.\n",
    "\n",
    "[2] Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed: “Random feedback weights support learning in deep neural networks”, 2014; arXiv:1411.0247.\n",
    "\n",
    "[3] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv: “Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1”, 2016; arXiv:1602.02830.\n",
    "\n",
    "[4] Paul Merolla, Rathinakumar Appuswamy, John Arthur, Steve K. Esser: “Deep neural networks are robust to weight binarization and other non-linear distortions”, 2016; arXiv:1606.01981."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
